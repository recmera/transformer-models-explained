# Explicación de artículos sobre modelos Transformer

### Index

1. [Attention Is All You Need](https://recmera.github.io/transformer-models-explained/transformer.html) ([paper](https://arxiv.org/abs/1706.03762))
2. [GPT (Generative Pre-trained Transformer)](https://recmera.github.io/transformer-models-explained/gtp.html) ([paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf))
3. [BERT (Bidirectional Encoder Representations from Transformers)](https://recmera.github.io/transformer-models-explained/bert.html) ([paper](https://arxiv.org/abs/1810.04805))
4. RoBERTa (Robustly Optimized BERT Pretraining Approach) ([paper](https://arxiv.org/abs/1907.11692))
5. T5 (Text-to-Text Transfer Transformer) ([paper](https://arxiv.org/abs/1910.10683))
6. XLNet (eXtreme MultiLingual Language Model) ([paper](https://arxiv.org/abs/1906.08237))
7. GShard ([paper](https://arxiv.org/abs/2006.16668))
8. Marian ([paper](https://arxiv.org/abs/1804.00344))
9. Reformer ([paper](https://arxiv.org/abs/2001.04451))
10. DeBERTa (Decoding-enhanced BERT with Disentangled Attention) ([paper](https://arxiv.org/abs/2006.03654))
11. ProphetNet
12. UniLM (Unified Language Model)
13. Longformer
14. Funnel Transformer
15. Performer
16. CTRL (Conditional Transformer Language Model)
17. ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)
18. BigBird
19. Routing Transformer
20. DistilBERT
21. ALBERT
22. CamemBERT
